{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO :**\n",
    "- tf-idf if not oom ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/theo/kaggle/foursquare/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.113095,
     "end_time": "2022-06-20T12:41:41.215410",
     "exception": false,
     "start_time": "2022-06-20T12:41:41.102315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"import os\\n\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"1\\\"\\n\\nimport torch\\n\\ntorch.cuda.get_device_name(0)\";\n",
       "                var nbb_formatted_code = \"import os\\n\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"1\\\"\\n\\nimport torch\\n\\ntorch.cuda.get_device_name(0)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-20T12:42:48.194197Z",
     "iopub.status.busy": "2022-06-20T12:42:48.193938Z",
     "iopub.status.idle": "2022-06-20T12:42:50.735561Z",
     "shell.execute_reply": "2022-06-20T12:42:50.734610Z"
    },
    "papermill": {
     "duration": 2.672311,
     "end_time": "2022-06-20T12:42:50.737847",
     "exception": false,
     "start_time": "2022-06-20T12:42:48.065536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"import gc\\nimport cudf\\nimport random\\nimport warnings\\nimport numpy as np\\nimport pandas as pd\\nfrom tqdm.auto import tqdm\\n\\nfrom params import DEBUG, OUT_PATH, IS_TEST, RESSOURCES_PATH\\nfrom ressources import *\\nfrom matching import *\\n\\nrandom.seed(13)\\nwarnings.simplefilter(\\\"ignore\\\")\\npd.options.display.max_columns = 500\";\n",
       "                var nbb_formatted_code = \"import gc\\nimport cudf\\nimport random\\nimport warnings\\nimport numpy as np\\nimport pandas as pd\\nfrom tqdm.auto import tqdm\\n\\nfrom params import DEBUG, OUT_PATH, IS_TEST, RESSOURCES_PATH\\nfrom ressources import *\\nfrom matching import *\\n\\nrandom.seed(13)\\nwarnings.simplefilter(\\\"ignore\\\")\\npd.options.display.max_columns = 500\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import cudf\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from params import DEBUG, OUT_PATH, IS_TEST, RESSOURCES_PATH\n",
    "from ressources import *\n",
    "from matching import *\n",
    "\n",
    "random.seed(13)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.128224,
     "end_time": "2022-06-20T12:42:52.888296",
     "exception": false,
     "start_time": "2022-06-20T12:42:52.760072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data\n",
    "- TODO: load precomputed probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"if IS_TEST:\\n    train = load_cleaned_data(OUT_PATH + \\\"cleaned_data_test.csv\\\")\\n    p1 = pd.read_csv(OUT_PATH + \\\"p1_yv_test.csv\\\")\\n    p2 = pd.read_csv(OUT_PATH + \\\"p2_yv_test.csv\\\")\\n\\n#     df_p = pd.read_csv(OUT_PATH + \\\"features_test_1.csv\\\")\\n\\nelse:\\n    train = load_cleaned_data(OUT_PATH + \\\"cleaned_data_train.csv\\\")\\n    p1 = pd.read_csv(OUT_PATH + \\\"p1_yv_train.csv\\\")\\n    p2 = pd.read_csv(OUT_PATH + \\\"p2_yv_train.csv\\\")\\n#     df_p = pd.read_csv(OUT_PATH + \\\"features_train_1.csv\\\")\";\n",
       "                var nbb_formatted_code = \"if IS_TEST:\\n    train = load_cleaned_data(OUT_PATH + \\\"cleaned_data_test.csv\\\")\\n    p1 = pd.read_csv(OUT_PATH + \\\"p1_yv_test.csv\\\")\\n    p2 = pd.read_csv(OUT_PATH + \\\"p2_yv_test.csv\\\")\\n\\n#     df_p = pd.read_csv(OUT_PATH + \\\"features_test_1.csv\\\")\\n\\nelse:\\n    train = load_cleaned_data(OUT_PATH + \\\"cleaned_data_train.csv\\\")\\n    p1 = pd.read_csv(OUT_PATH + \\\"p1_yv_train.csv\\\")\\n    p2 = pd.read_csv(OUT_PATH + \\\"p2_yv_train.csv\\\")\\n#     df_p = pd.read_csv(OUT_PATH + \\\"features_train_1.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if IS_TEST:\n",
    "    train = load_cleaned_data(OUT_PATH + \"cleaned_data_test.csv\")\n",
    "    p1 = pd.read_csv(OUT_PATH + \"p1_yv_test.csv\")\n",
    "    p2 = pd.read_csv(OUT_PATH + \"p2_yv_test.csv\")\n",
    "\n",
    "#     df_p = pd.read_csv(OUT_PATH + \"features_test_1.csv\")\n",
    "\n",
    "else:\n",
    "    train = load_cleaned_data(OUT_PATH + \"cleaned_data_train.csv\")\n",
    "    p1 = pd.read_csv(OUT_PATH + \"p1_yv_train.csv\")\n",
    "    p2 = pd.read_csv(OUT_PATH + \"p2_yv_train.csv\")\n",
    "#     df_p = pd.read_csv(OUT_PATH + \"features_train_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"df_p = pd.read_csv(OUT_PATH + \\\"features_train_1.csv\\\", usecols=[\\\"id_1\\\", \\\"id_2\\\"])\\n# df_p = df_p.sort_values([\\\"id_1\\\", \\\"id_2\\\"]).reset_index(drop=True)\";\n",
       "                var nbb_formatted_code = \"df_p = pd.read_csv(OUT_PATH + \\\"features_train_1.csv\\\", usecols=[\\\"id_1\\\", \\\"id_2\\\"])\\n# df_p = df_p.sort_values([\\\"id_1\\\", \\\"id_2\\\"]).reset_index(drop=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_p = pd.read_csv(OUT_PATH + \"features_train_1.csv\", usecols=[\"id_1\", \"id_2\"])\n",
    "# df_p = df_p.sort_values([\"id_1\", \"id_2\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8db8d3274daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mskiprows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mTHRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "skiprows = np.where(preds < THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_csv(OUT_PATH + \"features_train_1.csv\")\n",
    "df_p = df_p.sort_values([\"id_1\", \"id_2\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [\"../logs/lvl_1/2022-06-26/6/\"]\n",
    "\n",
    "preds = np.mean([np.load(f + \"pred_oof.npy\") for f in EXP_FOLDERS], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df_p[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    p1 = p1.head(100000).copy()\n",
    "    p2 = p2.head(100000).copy()\n",
    "    df_p = df_p.head(100000).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = list(df_p.columns[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"idx\"] = np.arange(len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youri & Vincent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fe import FE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = FE2(df_p, p1, p2, train, RESSOURCES_PATH) # call FE1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0, \"id_1\", p1[\"id\"].values)\n",
    "df.insert(1, \"id_2\", p2[\"id\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ThÃ©o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.features import (\n",
    "    is_equal,\n",
    "    is_included,\n",
    "    levenshtein,\n",
    "    wratio,\n",
    "    #     haversine_distance,\n",
    "    #     manhattan_distance,\n",
    "    #     angular_distance,\n",
    "    #     euclidian_distance,\n",
    "    #     angular_distance_l2,\n",
    "    compute_string_distance,\n",
    "    tf_idf_similarity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_string_distances(df, string_columns, verbose=0):\n",
    "    functions = [\n",
    "#         gesh,\n",
    "        levenshtein,\n",
    "#         wratio,\n",
    "    ]\n",
    "\n",
    "    features = []\n",
    "    for col in string_columns:\n",
    "        df[col + \"_1\"].fillna(\"\", inplace=True)\n",
    "        df[col + \"_2\"].fillna(\"\", inplace=True)\n",
    "\n",
    "        df[f\"{col}_len_1\"] = df[col + \"_1\"].apply(len)\n",
    "        df[f\"{col}_len_2\"] = df[col + \"_2\"].apply(len)\n",
    "\n",
    "        df[f\"{col}_len_diff\"] = np.abs(df[f\"{col}_len_1\"] - df[f\"{col}_len_2\"])\n",
    "        features.append(f\"{col}_len_diff\")\n",
    "\n",
    "        for fct in functions:\n",
    "            name = fct.__name__\n",
    "            if verbose:\n",
    "                print(f\"- Column : {col}  -  Function : {name}\")\n",
    "\n",
    "            df[col + \"_\" + name] = df[[col + \"_1\", col + \"_2\"]].apply(\n",
    "                lambda x: compute_string_distance(fct, x[0], x[1]), axis=1\n",
    "            )\n",
    "            features.append(col + \"_\" + name)\n",
    "\n",
    "        # Normalize\n",
    "        df[f\"{col}_levenshtein\"] = df[f\"{col}_levenshtein\"] / df[\n",
    "            [f\"{col}_len_1\", f\"{col}_len_2\"]\n",
    "        ].max(axis=1)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_position_distances(df, functions):\n",
    "    features = []\n",
    "\n",
    "    lats_1 = df[\"latitude_1\"]\n",
    "    longs_1 = df[\"longitude_1\"]\n",
    "    lats_2 = df[\"latitude_2\"]\n",
    "    longs_2 = df[\"longitude_2\"]\n",
    "\n",
    "    for fct in functions:\n",
    "        name = fct.__name__\n",
    "        df[name] = fct(lats_1, longs_1, lats_2, longs_2)\n",
    "        df[name + \"_s\"] = fct(longs_1, lats_1, lats_2, longs_2)\n",
    "        df[name + \"_min\"] = df[[name, name + \"_s\"]].min(1)\n",
    "\n",
    "        df.drop([name, name + \"_s\"], axis=1, inplace=True)\n",
    "\n",
    "        features += [name + \"_min\"]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_distance(lat1, long1, lat2, long2):\n",
    "    delta_long = pd.concat([np.abs(long2 - long1), 360 - np.abs(long2 - long1)], 1).min(\n",
    "        1\n",
    "    )\n",
    "    return np.abs(lat2 - lat1) + delta_long\n",
    "\n",
    "\n",
    "def angular_distance_l2(lat1, long1, lat2, long2):\n",
    "    delta_long = pd.concat([np.abs(long2 - long1), 360 - np.abs(long2 - long1)], 1).min(\n",
    "        1\n",
    "    )\n",
    "    return np.sqrt(np.clip((lat2 - lat1) ** 2 + delta_long**2, 0, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_theo(df, df_p):\n",
    "    features = []\n",
    "\n",
    "    for col in [\"name\", \"address\", \"url\"]:\n",
    "        df.loc[df[col] == \"\", col] = np.nan\n",
    "\n",
    "    #     print('- Computing nan features')\n",
    "    #     features += compute_nan_features(df_p, NAN_COLS)\n",
    "\n",
    "    #     for name, folder in NN_FT_FOLDERS:\n",
    "    #         print(f'- Adding features using model {name}')\n",
    "    #         nn_preds = np.load(OUT_PATH + f\"fts_{name}.npy\").astype(np.float16)\n",
    "    #         nn_preds = torch.from_numpy(nn_preds).cuda()\n",
    "\n",
    "    #         features += compute_nn_distances(df_p, nn_preds, suffix=\"_\" + name)\n",
    "\n",
    "    #         del nn_preds\n",
    "    #         gc.collect()\n",
    "    #         torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"- Computing position distances\")\n",
    "    DIST_FCTS = [angular_distance, angular_distance_l2]\n",
    "    features += compute_position_distances(df_p, DIST_FCTS)\n",
    "\n",
    "    #     TF_IDF_COLS = [\"name\", \"address\", \"url\"]\n",
    "    #     TF_IDF_PARAMS = [\n",
    "    #         ((3, 3), \"char_wb\"),  # char trigrams\n",
    "    #     ]\n",
    "    #     for col in TF_IDF_COLS:\n",
    "    #         for ngram_range, analyzer in TF_IDF_PARAMS:\n",
    "    #             ft_name = f\"{col}_tf_idf_{ngram_range[0]}{ngram_range[1]}_{analyzer}_sim\"\n",
    "    #             print(f\"- Computing feature {ft_name}\")\n",
    "\n",
    "    #             tf_idf = TfidfVectorizer(\n",
    "    #                 use_idf=False, ngram_range=ngram_range, analyzer=analyzer\n",
    "    #             )\n",
    "    #             tf_idf_mat = tf_idf.fit_transform(cudf.from_pandas(df[col].fillna(\"nan\")))\n",
    "\n",
    "    #             df_p[ft_name] = tf_idf_similarity(df_p, tf_idf_mat)\n",
    "    #             features.append(ft_name)\n",
    "\n",
    "    if not isinstance(df_p, pd.DataFrame):\n",
    "        df_p = df_p.to_pandas()\n",
    "\n",
    "    FEATURES_SAME = [\n",
    "        (\"state\", is_equal),\n",
    "        (\"zip\", is_included),\n",
    "        (\"city\", is_included),\n",
    "    ]\n",
    "\n",
    "    for col, fct in FEATURES_SAME:\n",
    "        print(f\"- Computing feature same_{col}\")\n",
    "        df_p[f\"same_{col}\"] = (\n",
    "            df_p[[f\"{col}_1\", f\"{col}_2\"]]\n",
    "            .fillna(\"\")\n",
    "            .apply(lambda x: fct(x[0], x[1]), axis=1)\n",
    "            .astype(float)  # parallel_apply\n",
    "        )\n",
    "\n",
    "        features.append(f\"same_{col}\")\n",
    "\n",
    "    STRING_DIST_COLS = [\"name\", \"address\", \"url\"]\n",
    "    features += compute_string_distances(df_p, STRING_DIST_COLS, verbose=1)\n",
    "\n",
    "    to_keep = [\"id_1\", \"id_2\"] + features\n",
    "    df_p.drop([c for c in df_p.columns if c not in to_keep], axis=1, inplace=True)\n",
    "\n",
    "    return df_p, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"address\",\n",
    "    \"country\",\n",
    "    \"url\",\n",
    "    \"phone\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"zip\",\n",
    "    \"categories\",\n",
    "    \"idx\",\n",
    "]\n",
    "pairs = pd.concat([p1[cols], p2[cols]], axis=1)\n",
    "pairs.columns = [c + \"_1\" for c in cols] + [c + \"_2\" for c in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_theo, fts_theo = feature_engineering_theo(train.copy(), pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df.merge(df_theo, on=[\"id_1\", \"id_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.scatter(df_merged[\"angular_distance_min\"], df_merged[\"dist_r1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corrs = df_merged.corr()\n",
    "for col in corrs.columns:\n",
    "    print(col, corrs.loc[corrs[col] > 0.9, col])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    if IS_TEST:\n",
    "        df_merged.to_csv(OUT_PATH + \"features_test_1.csv\", index=False)\n",
    "    else:\n",
    "        df_merged.to_csv(OUT_PATH + \"features_train_1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
